<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<HTML
><HEAD
><TITLE
>Трансляция журналов на резервные серверы</TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.79"><LINK
REV="MADE"
HREF="mailto:pgsql-docs@postgresql.org"><LINK
REL="HOME"
TITLE="Документация по PostgreSQL 9.4.1"
HREF="index.html"><LINK
REL="UP"
TITLE="Отказоустойчивость, балансировка нагрузки и репликация"
HREF="high-availability.html"><LINK
REL="PREVIOUS"
TITLE="Сравнение различных решений"
HREF="different-replication-solutions.html"><LINK
REL="NEXT"
TITLE="Отработка отказа"
HREF="warm-standby-failover.html"><LINK
REL="STYLESHEET"
TYPE="text/css"
HREF="stylesheet.css"><META
HTTP-EQUIV="Content-Type"
CONTENT="text/html; charset=UTF-8"><META
NAME="creation"
CONTENT="2016-04-12T07:56:57"></HEAD
><BODY
CLASS="SECT1"
><DIV
CLASS="NAVHEADER"
><TABLE
SUMMARY="Header navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TH
COLSPAN="4"
ALIGN="center"
VALIGN="bottom"
><A
HREF="index.html"
>Документация по PostgreSQL 9.4.1</A
></TH
></TR
><TR
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="top"
><A
TITLE="Сравнение различных решений"
HREF="different-replication-solutions.html"
ACCESSKEY="P"
>Пред.</A
></TD
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="top"
><A
HREF="high-availability.html"
ACCESSKEY="U"
>Уровень выше</A
></TD
><TD
WIDTH="60%"
ALIGN="center"
VALIGN="bottom"
>Глава 25. Отказоустойчивость, балансировка нагрузки и репликация</TD
><TD
WIDTH="20%"
ALIGN="right"
VALIGN="top"
><A
TITLE="Отработка отказа"
HREF="warm-standby-failover.html"
ACCESSKEY="N"
>След.</A
></TD
></TR
></TABLE
><HR
ALIGN="LEFT"
WIDTH="100%"></DIV
><DIV
CLASS="SECT1"
><H1
CLASS="SECT1"
><A
NAME="WARM-STANDBY"
>25.2. Трансляция журналов на резервные серверы</A
></H1
><P
>Постоянная архивация может использоваться для создания кластерной конфигурации <I
CLASS="FIRSTTERM"
>высокой степени доступности</I
> (HA) с одним или несколькими <I
CLASS="FIRSTTERM"
>резервными серверами</I
>, способными заменить ведущий сервер в случае выхода его из строя. Такую реализацию отказоустойчивости часто называют <I
CLASS="FIRSTTERM"
>тёплый резерв</I
> или <I
CLASS="FIRSTTERM"
>трансляция журналов</I
>.</P
><P
>Ведущий и резервный серверы работают совместно для обеспечения этой возможности, при этом они связаны опосредованно. Ведущий сервер работает в режиме постоянной архивации изменений, в то время как каждый резервный сервер работает в режиме постоянного приема архивных изменений, зачитывая WAL-файлы с ведущего. Для обеспечения этой возможности не требуется вносить изменения в таблицы БД, что требует существенно меньших административных издержек в сравнении с некоторыми другими решениями репликации. Так же такая конфигурация относительно слабо влияет на производительность ведущего сервера.</P
><P
>Непосредственную передачу записей WAL с одного сервера БД на другой обычно называют трансляцией журналов (или доставкой журналов). <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> реализует трансляцию журналов на уровне файлов, передавая записи WAL по одному файлу (сегменту WAL) единовременно. Файлы WAL (размером 16 МБ) можно легко и эффективно передать на любое расстояние, будь то соседний сервер, другая система в местной сети или сервер на другом краю света. Требуемая пропускная способность при таком подходе определяется скоростью записи транзакций на ведущем сервере. Трансляция журналов на уровне записей более фрагментарная операция, при которой изменения WAL передаются последовательно через сетевое соединение (см. <A
HREF="warm-standby.html#STREAMING-REPLICATION"
>Подраздел 25.2.5</A
>).</P
><P
>Следует отметить, что трансляция журналов асинхронна, то есть записи WAL доставляются после завершения транзакции. В результате образуется окно, когда возможна потеря данных при отказе сервера: будут утеряны ещё не переданные транзакции. Размер этого окна при трансляции файлов журналов может быть ограничен параметром <TT
CLASS="VARNAME"
>archive_timeout</TT
>, который может принимать значение меньше нескольких секунд. Тем не менее подобные заниженные значения могут потребовать существенного увеличения пропускной способности, необходимой для трансляции файлов. При потоковой репликации (см. <A
HREF="warm-standby.html#STREAMING-REPLICATION"
>Подраздел 25.2.5</A
>) окно возможности потери данных гораздо меньше.</P
><P
>Скорость восстановления достаточно высока, обычно резервный сервер становится полностью доступным через мгновение после активации. В результате такое решение называется тёплым резервом, что обеспечивает отличную отказоустойчивость. Восстановление сервера из архивной копии базы и применение изменений обычно происходит существенно дольше. Поэтому такие действия обычно требуются при восстановлении после аварии, не для отказоустойчивости. Так же резервный сервер может обрабатывать читающие запросы. В этом случае он называется сервером горячего резерва. См. <A
HREF="hot-standby.html"
>Раздел 25.5</A
> для подробной информации.</P
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="STANDBY-PLANNING"
>25.2.1. Планирование</A
></H2
><P
>Обычно разумно подбирать ведущий и резервный серверы так, чтобы они были максимально похожи, как минимум с точки зрения базы данных. Тогда в частности, пути, связанные с табличными пространствами, могут передаваться без изменений. Таким образом, как на ведущем, так и на резервных серверах должны быть одинаковые пути монтирования для табличных пространств при использовании этой возможности БД. Учитывайте, что если <A
HREF="sql-createtablespace.html"
>CREATE TABLESPACE</A
> выполнена на ведущем сервере, новая точка монтирования для этой команды уже должна существовать на резервных серверах до её выполнения. Аппаратная часть не должна быть в точности одинаковой, но опыт показывает, что сопровождать идентичные системы легче, чем две различные на протяжении жизненного цикла приложения и системы. В любом случае архитектура оборудования должна быть одинаковой &mdash; например, трансляция журналов с 32-битной на 64-битную систему не будет работать.</P
><P
>В общем случае трансляция журналов между серверами с различными основными версиями <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> невозможна. Политика главной группы разработки PostgreSQL состоит в том, чтобы не вносить изменения в дисковые форматы при обновлениях корректирующей версии, таким образом, ведущий и резервный серверы, имеющие разные корректирующие версии, могут работать успешно. Тем не менее, формально такая возможность не поддерживается и рекомендуется поддерживать одинаковую версию ведущего и резервных серверов, насколько это возможно. При обновлении корректирующей версии безопаснее будет в первую очередь обновить резервные серверы &mdash; новая корректирующая версия с большей вероятностью прочитает файл WAL предыдущей корректирующей версии, чем наоборот.</P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="STANDBY-SERVER-OPERATION"
>25.2.2. Работа резервного сервера</A
></H2
><P
>Сервер, работающий в режиме резервного, последовательно применяет файлы WAL, полученные от главного. Резервный сервер может читать файлы WAL из архива WAL (см. <A
HREF="archive-recovery-settings.html#RESTORE-COMMAND"
>restore_command</A
>) или напрямую с главного сервера по соединению TCP (потоковая репликация). Резервный сервер так же будет пытаться восстановить любой файл WAL, найденный в кластере резервного в каталоге <TT
CLASS="FILENAME"
>pg_xlog</TT
>. Это обычно происходит после перезапуска сервера, когда он применяет заново файлы WAL, полученные от главного сервера перед перезапуском. Но можно и вручную скопировать файлы в каталог <TT
CLASS="FILENAME"
>pg_xlog</TT
>, чтобы применить их в любой момент времени.</P
><P
>В момент запуска резервный сервер начинает восстанавливать все доступные файлы WAL, размещённые в архивном каталоге, указанном в команде <TT
CLASS="VARNAME"
>restore_command</TT
>. По достижении конца доступных файлов WAL или при сбое команды <TT
CLASS="VARNAME"
>restore_command</TT
> сервер пытается восстановить все файлы WAL, доступные в каталоге <TT
CLASS="FILENAME"
>pg_xlog</TT
>. Если это не удаётся и потоковая репликация настроена, резервный сервер пытается присоединиться к ведущему и начать закачивать поток WAL с последней подтверждённой записи, найденной в архиве или <TT
CLASS="FILENAME"
>pg_xlog</TT
>. Если это действие закончилось неудачей, или потоковая репликация не настроена, или соединение позднее разорвалось, резервный сервер возвращается к шагу 1 и пытается восстановить файлы из архива вновь. Цикл обращения за файлами WAL к архиву, <TT
CLASS="FILENAME"
>pg_xlog</TT
>, и через потоковую репликацию продолжается до остановки сервера или переключения его роли, вызванного файлом-триггером.</P
><P
>Режим резерва завершается и сервер переключается в обычный рабочий режим при получении команды <TT
CLASS="COMMAND"
>pg_ctl promote</TT
> или при обнаружении файла-триггера (<TT
CLASS="VARNAME"
>trigger_file</TT
>). Перед переключением сервер восстановит все файлы WAL, непосредственно доступные из архива или <TT
CLASS="FILENAME"
>pg_xlog</TT
>, но пытаться подключиться к главному серверу он больше не будет.</P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="PREPARING-MASTER-FOR-STANDBY"
>25.2.3. Подготовка главного сервера для работы с резервными</A
></H2
><P
>Настройка постоянного архивирования на ведущем сервере в архивный каталог, доступный с резервного, описана в <A
HREF="continuous-archiving.html"
>Разделе 24.3</A
>. Расположение архива должно быть доступно с резервного сервера даже при отключении главного, то есть его следует разместить на резервном или другом доверенном, но не на главном сервере.</P
><P
>При использовании потоковой репликации следует настроить режим аутентификации на ведущем сервере, чтобы разрешить соединения с резервных. Для этого создать роль и обеспечить подходящую запись в файле <TT
CLASS="FILENAME"
>pg_hba.conf</TT
> в разделе доступа к БД <TT
CLASS="LITERAL"
>replication</TT
>. Так же следует убедиться, что для параметра <TT
CLASS="VARNAME"
>max_wal_senders</TT
> задаётся достаточно большое значение в конфигурационном файле ведущего сервера. При использовании слотов для репликации также достаточно большое значение нужно задать для <TT
CLASS="VARNAME"
>max_replication_slots</TT
>.</P
><P
>Создание базовой резервной копии, необходимой для запуска резервного сервера, описано в <A
HREF="continuous-archiving.html#BACKUP-BASE-BACKUP"
>Подразделе 24.3.2</A
>.</P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="STANDBY-SERVER-SETUP"
>25.2.4. Настройка резервного сервера</A
></H2
><P
>Для запуска резервного сервера нужно восстановить резервную копию, снятую с ведущего (см. <A
HREF="continuous-archiving.html#BACKUP-PITR-RECOVERY"
>Подраздел 24.3.4</A
>). Затем нужно создать файл команд восстановления <TT
CLASS="FILENAME"
>recovery.conf</TT
> в каталоге данных кластера резервного сервера и включить режим <TT
CLASS="VARNAME"
>standby_mode</TT
>. Задайте в <TT
CLASS="VARNAME"
>restore_command</TT
> обычную команду копирования файлов из архива WAL. Если планируется несколько резервных серверов в целях отказоустойчивости, установите для <TT
CLASS="VARNAME"
>recovery_target_timeline</TT
> значение <TT
CLASS="LITERAL"
>latest</TT
>, чтобы резервный сервер переходил на новую линию времени, образуемую при отработке отказа и переключении на другой сервер.</P
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Замечание: </B
>Не используйте pg_standby или подобные средства со встроенным режимом резервного сервера, описанным здесь. <TT
CLASS="VARNAME"
>restore_command</TT
> должна немедленно прекратиться при отсутствии файла; сервер повторит команду вновь при необходимости. Использование средств, подобных pg_standby, описано в <A
HREF="log-shipping-alternative.html"
>Разделе 25.4</A
>.</P
></BLOCKQUOTE
></DIV
><P
>При необходимости потоковой репликации заполните <TT
CLASS="VARNAME"
>primary_conninfo</TT
> параметрами строки соединения для libpq, включая имя (или IP-адрес) сервера и все остальные необходимые данные для соединения с ведущим сервером. Если ведущий требует пароль для аутентификации, пароль может быть так же передан в <TT
CLASS="VARNAME"
>primary_conninfo</TT
>.</P
><P
>Если резервный сервер настраивается в целях отказоустойчивости, на нём следует настроить архивацию WAL, соединения и аутентификацию, как на ведущем сервере, потому что резервный сервер станет ведущим после отработки отказа.</P
><P
>При использовании архива WAL его размер может быть уменьшен с помощью команды в параметре <A
HREF="archive-recovery-settings.html#ARCHIVE-CLEANUP-COMMAND"
>archive_cleanup_command</A
>, которая удаляет файлы уже не нужные для дальнейшей работы резервного сервера. Утилита <SPAN
CLASS="APPLICATION"
>pg_archivecleanup</SPAN
> разработана специально для использования в <TT
CLASS="VARNAME"
>archive_cleanup_command</TT
> при типичной конфигурации с одним резервным сервером (см. <A
HREF="pgarchivecleanup.html"
>              <SPAN
CLASS="APPLICATION"
>pg_archivecleanup</SPAN
>
            </A
>). Следует отметить, что если архив используется в целях резервирования, следует сохранять все файлы необходимые для восстановления как минимум с последней базовой резервной копии, даже если они не нужны для резервного сервера.</P
><P
>Простой пример <TT
CLASS="FILENAME"
>recovery.conf</TT
>: </P><PRE
CLASS="PROGRAMLISTING"
>standby_mode = 'on'
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
restore_command = 'cp /path/to/archive/%f %p'
archive_cleanup_command = 'pg_archivecleanup /path/to/archive %r'</PRE
><P></P
><P
>Можно поддерживать любое количество резервных серверов, но при применении потоковой репликации необходимо убедиться, что значение <TT
CLASS="VARNAME"
>max_wal_senders</TT
> на ведущем достаточно большое, чтобы все они могли подключиться одновременно.</P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="STREAMING-REPLICATION"
>25.2.5. Потоковая репликация</A
></H2
><P
>При потоковой репликации резервный сервер может работать с меньшей задержкой, чем при трансляции файлов. Резервный сервер подключается к ведущему, который передаёт поток записей WAL резервному в момент их добавления, не дожидаясь окончания заполнения файла WAL.</P
><P
>Потоковая репликация асинхронна по умолчанию (см. <A
HREF="warm-standby.html#SYNCHRONOUS-REPLICATION"
>Подраздел 25.2.8</A
>), то есть имеется небольшая задержка между подтверждением транзакции на ведущем сервере и появлением этих изменений на резервном. Тем не менее, эта задержка гораздо меньше, чем при трансляции файлов журналов, обычно в пределах одной секунды, если резервный сервер достаточно мощный и справляется с нагрузкой. При потоковой репликации настраивать <TT
CLASS="VARNAME"
>archive_timeout</TT
> для уменьшения окна потенциальной потери данных не требуется.</P
><P
>При потоковой репликации без постоянной архивации на уровне файлов, сервер может избавиться от старых сегментов WAL до того, как резервный получит их. В этом случае резервный сервер потребует повторной инициализации из новой базовой резервной копии. Этого можно избежать, установив для <TT
CLASS="VARNAME"
>wal_keep_segments</TT
> достаточно большое значение, при котором сегменты WAL будут защищены от ранней очистки, либо настроив слот репликации для резервного сервера. Если с резервного сервера доступен архив WAL, этого не требуется, так как резервный может всегда обратиться к архиву для восполнения пропущенных сегментов.</P
><P
>Чтобы включить потоковую репликацию, сначала настройте резервный сервер на приём трансляции журналов, как описано в <A
HREF="warm-standby.html"
>Разделе 25.2</A
>. Затем сделайте следующий шаг — переключите резервный сервер в режим репликации, установив в <TT
CLASS="VARNAME"
>primary_conninfo</TT
> в файле <TT
CLASS="FILENAME"
>recovery.conf</TT
> строку подключения, указывающую на ведущий. Настройте <A
HREF="runtime-config-connection.html#GUC-LISTEN-ADDRESSES"
>listen_addresses</A
> и параметры аутентификации (см. <TT
CLASS="FILENAME"
>pg_hba.conf</TT
>) на ведущем сервере таким образом, чтобы резервный смог подключиться к псевдобазе <TT
CLASS="LITERAL"
>replication</TT
> на ведущем (см. <A
HREF="warm-standby.html#STREAMING-REPLICATION-AUTHENTICATION"
>Подраздел 25.2.5.1</A
>).</P
><P
>В системах, поддерживающих параметр сокета keepalive, подходящие значения <A
HREF="runtime-config-connection.html#GUC-TCP-KEEPALIVES-IDLE"
>tcp_keepalives_idle</A
>, <A
HREF="runtime-config-connection.html#GUC-TCP-KEEPALIVES-INTERVAL"
>tcp_keepalives_interval</A
> и <A
HREF="runtime-config-connection.html#GUC-TCP-KEEPALIVES-COUNT"
>tcp_keepalives_count</A
> помогут ведущему вовремя заметить разрыв соединения.</P
><P
>Установите максимальное количество одновременных соединений с резервных серверов (см. описание <A
HREF="runtime-config-replication.html#GUC-MAX-WAL-SENDERS"
>max_wal_senders</A
>.</P
><P
>При запуске резервного сервера с правильно установленным <TT
CLASS="VARNAME"
>primary_conninfo</TT
> резервный подключится к ведущему после воспроизведения всех файлов WAL, доступных из архива. При успешном установлении соединения можно увидеть процесс walreceiver на резервном сервере и соответствующий процесс walsender на ведущем.</P
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="STREAMING-REPLICATION-AUTHENTICATION"
>25.2.5.1. Аутентификация</A
></H3
><P
>Следует особо отметить, что привилегию доступа к репликации следует устанавливать только для пользователей с высоким уровнем доступа, так как они могут читать поток WAL, из которого легко извлечь доверенную информацию. Резервный сервер должен аутентифицироваться на ведущем от имени суперпользователя или от учётной записи с привилегией <TT
CLASS="LITERAL"
>REPLICATION</TT
>. Настоятельно рекомендуется создавать выделенного пользователя с привилегиями <TT
CLASS="LITERAL"
>REPLICATION</TT
> и <TT
CLASS="LITERAL"
>LOGIN</TT
> специально для репликации. Хотя привилегия <TT
CLASS="LITERAL"
>REPLICATION</TT
> даёт очень широкие полномочия, она не позволяет модифицировать данные в ведущей системе, тогда как с привилегией <TT
CLASS="LITERAL"
>SUPERUSER</TT
> это можно делать.</P
><P
>Список аутентификации клиентов для репликации содержится в <TT
CLASS="FILENAME"
>pg_hba.conf</TT
> в записях с установленным значением <TT
CLASS="LITERAL"
>replication</TT
> в поле <TT
CLASS="REPLACEABLE"
><I
>database</I
></TT
>. Например, если резервный сервер запущен на компьютере с IP-адресом <TT
CLASS="LITERAL"
>192.168.1.100</TT
> и учётная запись для репликации <TT
CLASS="LITERAL"
>foo</TT
>, администратор может добавить следующую строку в файл <TT
CLASS="FILENAME"
>pg_hba.conf</TT
> ведущего: </P><PRE
CLASS="PROGRAMLISTING"
># Разрешить пользователю "foo" с компьютера 192.168.1.100 подключаться к этому
# серверу в качестве партнёра репликации, если был передан правильный пароль.
#
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    replication     foo             192.168.1.100/32        md5</PRE
><P></P
><P
>Имя компьютера и номер порта для ведущего, имя пользователя для соединения и пароль указываются в файле <TT
CLASS="FILENAME"
>recovery.conf</TT
>. Пароль так же может быть задан через файл <TT
CLASS="FILENAME"
>~/.pgpass</TT
> на резервном сервере (указанном в определении с <TT
CLASS="LITERAL"
>replication</TT
> в поле <TT
CLASS="REPLACEABLE"
><I
>database</I
></TT
>). Например, если ведущий принимает подключения по IP-адресу <TT
CLASS="LITERAL"
>192.168.1.50</TT
>, в порту <TT
CLASS="LITERAL"
>5432</TT
>, пользователя для репликации <TT
CLASS="LITERAL"
>foo</TT
> с паролем <TT
CLASS="LITERAL"
>foopass</TT
>, то администратор может добавить следующую строку в файл <TT
CLASS="FILENAME"
>recovery.conf</TT
> на резервном сервере: </P><PRE
CLASS="PROGRAMLISTING"
># Резервный сервер подключается к ведущему, работающему на компьютере 192.168.1.50
# (порт 5432), от имени пользователя "foo" с паролем "foopass".
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'</PRE
><P></P
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="STREAMING-REPLICATION-MONITORING"
>25.2.5.2. Наблюдение</A
></H3
><P
>Важным индикатором стабильности работы потоковой репликации является количество записей WAL, созданных на ведущем, но ещё не применённых на резервном сервере. Можно подсчитать задержку, сравнив текущий WAL, записанный на ведущем, с последним WAL, полученным на резервном. Эти показатели могут быть получены с помощью функций <CODE
CLASS="FUNCTION"
>pg_current_xlog_location</CODE
> на ведущем и <CODE
CLASS="FUNCTION"
>pg_last_xlog_receive_location</CODE
> на резервном соответственно (более подробно см. <A
HREF="functions-admin.html#FUNCTIONS-ADMIN-BACKUP-TABLE"
>Таблицу 9-65</A
> и <A
HREF="functions-admin.html#FUNCTIONS-RECOVERY-INFO-TABLE"
>Таблицу 9-66</A
>). Местоположение последнего полученного WAL на резервном сервере так же показывается в статусе процесса получателя WAL, отображаемого по команде <TT
CLASS="COMMAND"
>ps</TT
> (более подробно см. <A
HREF="monitoring-ps.html"
>Раздел 27.1</A
>).</P
><P
>Можно запросить список процессов отправителей WAL через представление <A
HREF="monitoring-stats.html#MONITORING-STATS-VIEWS-TABLE"
><TT
CLASS="LITERAL"
>pg_stat_replication</TT
></A
>. Большая разница между <CODE
CLASS="FUNCTION"
>pg_current_xlog_location</CODE
> и полем <TT
CLASS="LITERAL"
>sent_location</TT
> указывает на то, что главный сервер работает с большой нагрузкой, в то время как разница между <TT
CLASS="LITERAL"
>sent_location</TT
> и <CODE
CLASS="FUNCTION"
>pg_last_xlog_receive_location</CODE
> на резервном указывает на задержки в сети или на то, что с большой нагрузкой работает резервный.</P
></DIV
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="STREAMING-REPLICATION-SLOTS"
>25.2.6. Слоты репликации</A
></H2
><P
>Слоты репликации автоматически обеспечивают механизм сохранения сегментов WAL, пока они не будут получены всеми резервными и главный сервер не будет удалять строки, находящиеся в статусе <A
HREF="hot-standby.html#HOT-STANDBY-CONFLICT"
>recovery conflict</A
> даже при отключении резервного.</P
><P
>Вместо использования слотов репликации для предотвращения удаления старых сегментов WAL можно применять <A
HREF="runtime-config-replication.html#GUC-WAL-KEEP-SEGMENTS"
>wal_keep_segments</A
>, или сохранять сегменты в архиве с помощью команды <A
HREF="runtime-config-wal.html#GUC-ARCHIVE-COMMAND"
>archive_command</A
>. Тем не менее, эти методы часто приводят к тому, что хранится больше сегментов WAL, чем необходимо, в то время как слоты репликации оставляют только то количество сегментов, которое необходимо. Преимущество этих методов состоит в том, что они чётко задают объёмы места, необходимого для <TT
CLASS="LITERAL"
>pg_xlog</TT
>; в то время как текущая реализация репликационных слотов не представляет такой возможности.</P
><P
>Подобным образом, параметры <A
HREF="runtime-config-replication.html#GUC-HOT-STANDBY-FEEDBACK"
>hot_standby_feedback</A
> и <A
HREF="runtime-config-replication.html#GUC-VACUUM-DEFER-CLEANUP-AGE"
>vacuum_defer_cleanup_age</A
> позволяют защитить востребованные строки от удаления при очистке, но первый параметр не защищает в тот промежуток времени, когда резервный сервер не подключён, а для последнего часто нужно задавать большое значение, чтобы обеспечить должную защиту. Слоты репликации решают эти проблемы.</P
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="STREAMING-REPLICATION-SLOTS-MANIPULATION"
>25.2.6.1. Запросы и действия слотов репликации</A
></H3
><P
>Каждый слот репликации обладает именем, состоящим из строчных букв, цифр и символов подчёркивания.</P
><P
>Имеющиеся слоты репликации и их статус можно просмотреть в представлении <A
HREF="catalog-pg-replication-slots.html"
><TT
CLASS="STRUCTNAME"
>pg_replication_slots</TT
></A
>.</P
><P
>Слоты могут быть созданы и удалены как с помощью протокола потоковой репликации (см. <A
HREF="protocol-replication.html"
>Раздел 49.3</A
>), так и посредством функций SQL (см. <A
HREF="functions-admin.html#FUNCTIONS-REPLICATION"
>Подраздел 9.26.6</A
>).</P
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="STREAMING-REPLICATION-SLOTS-CONFIG"
>25.2.6.2. Пример конфигурации</A
></H3
><P
>Для создания слота репликации выполните: </P><PRE
CLASS="PROGRAMLISTING"
>postgres=# SELECT * FROM pg_create_physical_replication_slot('node_a_slot');
  slot_name  | xlog_position
-------------+---------------
 node_a_slot |

postgres=# SELECT * FROM pg_replication_slots;
  slot_name  | slot_type | datoid | database | active | xmin | restart_lsn
-------------+-----------+--------+----------+--------+------+-------------
 node_a_slot | physical  |        |          | f      |      |
(1 row)</PRE
><P> Для настройки резервного сервера на использование этого слота <TT
CLASS="VARNAME"
>primary_slot_name</TT
> должно быть настроено в конфигурации <TT
CLASS="FILENAME"
>recovery.conf</TT
> резервного. Вот простейший пример: </P><PRE
CLASS="PROGRAMLISTING"
>standby_mode = 'on'
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
primary_slot_name = 'node_a_slot'</PRE
><P></P
></DIV
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="CASCADING-REPLICATION"
>25.2.7. Каскадная репликация</A
></H2
><P
>Свойство каскадной репликации позволяет резервному серверу принимать соединения репликации и потоки WAL от других резервных, выступающих посредниками. Это может быть полезно для уменьшения числа непосредственных подключений к главному серверу, а также для уменьшения накладных расходов при передаче данных в интрасети.</P
><P
>Резервный сервер, выступающий как получатель и отправитель, называется каскадным резервным сервером. Резервные серверы, стоящие ближе к главному, называются серверами верхнего уровня, а более отдалённые — серверами нижнего уровня. Каскадная репликация не накладывает ограничений на количество или организацию последующих уровней, а каждый резервный соединяется только с одним сервером вышестоящего уровня, который в конце концов соединяется с единственным главным/ведущим сервером.</P
><P
>Резервный сервер каскадной репликации не только получает записи WAL от главного, но так же восстанавливает их из архива. Таким образом, даже если соединение с сервером более высокого уровня разорвётся, потоковая репликация для последующих уровней будет продолжаться до исчерпания доступных записей WAL.</P
><P
>Каскадная репликация в текущей реализации асинхронна. Параметры синхронной репликации (см. <A
HREF="warm-standby.html#SYNCHRONOUS-REPLICATION"
>Подраздел 25.2.8</A
>) в настоящее время не оказывают влияние на каскадную репликацию.</P
><P
>Распространение обратной связи горячего резерва работает от нижестоящего уровня к вышестоящему уровню вне зависимости от способа организации связи.</P
><P
>Если резервный сервер вышестоящего уровня будет преобразован в новый главный, серверы нижестоящего уровня продолжат получать поток с нового главного при условии, что <TT
CLASS="VARNAME"
>recovery_target_timeline</TT
> установлен в значение <TT
CLASS="LITERAL"
>'latest'</TT
>.</P
><P
>Для использования каскадной репликации необходимо настроить резервный каскадный сервер на прием соединений репликации (то есть установить <A
HREF="runtime-config-replication.html#GUC-MAX-WAL-SENDERS"
>max_wal_senders</A
> и <A
HREF="runtime-config-replication.html#GUC-HOT-STANDBY"
>hot_standby</A
>, настроить <A
HREF="auth-pg-hba-conf.html"
>host-based authentication</A
>). Так же может быть необходимо настроить на нижестоящем резервном значение <TT
CLASS="VARNAME"
>primary_conninfo</TT
> на каскадный резервный сервер.</P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="SYNCHRONOUS-REPLICATION"
>25.2.8. Синхронная репликация</A
></H2
><P
>По умолчанию в <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> потоковая репликация асинхронна. Если ведущий сервер выходит из строя, некоторые транзакции, которые были подтверждены, но не переданы на резервный, могут быть потеряны. Объём потерянных данных пропорционален задержке репликации на момент отработки отказа.</P
><P
>Синхронная репликация предоставляет возможность гарантировать, что все изменения, внесённые в транзакции, были переданы одному синхронному резервному серверу. Это увеличивает стандартный уровень надёжности, гарантируемый при фиксации транзакции. Этот уровень защиты соответствует второму уровню безопасности репликации из теории вычислительной техники.</P
><P
>При задействовании синхронной репликации, каждое подтверждение пишущей транзакции ожидает подтверждения того, что транзакция записана в журнал транзакций на диске на обоих серверах: ведущем и резервном. При таком варианте потеря данных может произойти, только в случае одновременного выхода из строя ведущего и резервного серверов. Это обеспечивает более высокий уровень надёжности, но только в случае внимательной работы системного администратора при установке и настройке этих двух серверов. Ожидание подтверждения увеличивает уверенность в том, что данные не будут потеряны во время сбоя сервера, но при этом увеличивает время отклика для обработки транзакции. Минимальное время ожидания равно времени передачи данных от ведущего к резервному и обратно.</P
><P
>Транзакции только для чтения и откат транзакции не требуют ожидания для ответа с резервного сервера. Промежуточные подтверждения не ожидают ответа от резервного сервера, только подтверждение верхнего уровня. Долгие операции вида загрузки данных или построения индекса не ожидают финального подтверждения. Но все двухфазные подтверждения требуют ожидания, включая подготовку и непосредственно подтверждение.</P
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="SYNCHRONOUS-REPLICATION-CONFIG"
>25.2.8.1. Базовая настройка</A
></H3
><P
>При настроенной потоковой репликации установка синхронной репликации требует только дополнительной настройки: необходимо выставить <A
HREF="runtime-config-replication.html#GUC-SYNCHRONOUS-STANDBY-NAMES"
>synchronous_standby_names</A
> в непустое значение. Так же необходимо установить <TT
CLASS="VARNAME"
>synchronous_commit</TT
> в значение <TT
CLASS="LITERAL"
>on</TT
>, но так как это значение по умолчанию, обычно действий не требуется. (См. <A
HREF="runtime-config-wal.html#RUNTIME-CONFIG-WAL-SETTINGS"
>Подраздел 18.5.1</A
> и <A
HREF="runtime-config-replication.html#RUNTIME-CONFIG-REPLICATION-MASTER"
>Подраздел 18.6.2</A
>.) В такой конфигурации каждая транзакция будет ожидать подтверждение того, что на резервном сервере произошла запись транзакции в надёжное хранилище. Значение <TT
CLASS="VARNAME"
>synchronous_commit</TT
> может быть выставлено для отдельного пользователя, может быть прописано в файле конфигурации, для конкретного пользователя или БД или динамически изменено приложением для управления степенью надёжности на уровне отдельных транзакций.</P
><P
>После подтверждения транзакции происходит запись на диск на ведущем сервере, запись WAL отправляется на резервный. Резервный сервер посылает ответ каждый раз, когда новый пакет данных WAL записан на диск, даже если значение <TT
CLASS="VARNAME"
>wal_receiver_status_interval</TT
> установлено в 0 на резервном. Первый резервный сервер, соответствущий по имени значению из <TT
CLASS="VARNAME"
>synchronous_standby_names</TT
>, установленному на ведущем и ответивший подтверждением записи, позволяет считать ведущему, что подтверждение получено. Этот параметр позволяет администратору указать, который из резервных серверов будет участвовать в синхронной репликации. Следует отметить, что настройка синхронной репликации происходит по большей части на главном сервере. Названный резервный сервер должен быть напрямую соединён с главным, так как главный ничего не знает о резервных более низкого уровня, задействованных в каскадной репликации.</P
><P
>Если <TT
CLASS="VARNAME"
>synchronous_commit</TT
> имеет значение <TT
CLASS="LITERAL"
>remote_write</TT
>, то в случае подтверждения транзакции ответ от резервного сервера об успешном подтверждении будет передан, когда данные запишутся в операционной системе, но не когда данные будет реально сохранены на диске. При таком значении уровень надёжности снижается по сравнению со значением <TT
CLASS="LITERAL"
>on</TT
>. Резервный сервер может потерять данные в случае падения операционной системы, но не в случае падения <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>. Тем не менее, этот вариант полезен на практике, так как позволяет сократить время отклика для транзакции. Потеря данных может произойти только в случае одновременного сбоя ведущего и резервного, осложнённого повреждением БД на ведущем.</P
><P
>Пользователи прекратят ожидание в случае запроса на быструю остановку сервера. В то время как при использовании асинхронной репликации сервер не будет полностью остановлен, пока все исходящие записи WAL не переместятся на текущий присоединённый резервный сервер.</P
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="SYNCHRONOUS-REPLICATION-PERFORMANCE"
>25.2.8.2. Планирование производительности</A
></H3
><P
>Синхронная репликация обычно требует более тщательного планирования и размещения резервных серверов для обеспечения приемлемой производительности. Ожидания не потребляют системные ресурсы, но транзакционные блокировки налагаются до окончания подтверждения. Как следствие, непродуманное использование синхронной репликации уменьшает производительность БД по причине увеличения времени отклика и повышения конкуренции за ресурсы.</P
><P
><SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> позволяет разработчикам выбрать требуемый уровень надёжности, обеспечиваемый при репликации. Он может быть установлен для системы в целом, для отдельного пользователя или соединения или даже для отдельной транзакции.</P
><P
>Например, в процессе жизненного цикла приложения 10% изменений являются важными данными клиентов, другие 90% изменений менее важны для бизнеса и могут быть легко восстановлены при их потере (например, чат между пользователями)</P
><P
>При настройке уровня синхронности репликации на уровне приложения (на ведущем) можно задать синхронную репликацию для большинства важных изменений без замедления общего рабочего ритма. Возможность настройки на уровне приложения является важным и практичным средством для получения выгод синхронной репликации при высоком быстродействии.</P
><P
>Следует иметь в виду, что пропускная способность сети должна быть выше плотности потока генерируемых данных WAL.</P
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="SYNCHRONOUS-REPLICATION-HA"
>25.2.8.3. Планирование отказоустойчивости</A
></H3
><P
>Если транзакция фиксируется в режиме <TT
CLASS="VARNAME"
>synchronous_commit</TT
> <TT
CLASS="LITERAL"
>on</TT
> или <TT
CLASS="LITERAL"
>remote_write</TT
>, ведущий сервер будет ждать ответа синхронного резервного. Этот ответ может быть не получен никогда, если последний (или единственный) резервный сервер выйдет из строя.</P
><P
>Лучший способ избежать потери данных — защититься от потери последнего из синхронных резервных серверов. Этого можно добиться, перечислив имена нескольких потенциальных синхронных резервных серверов в строке <TT
CLASS="VARNAME"
>synchronous_standby_names</TT
>. Изначально в качестве синхронного резервного сервера будет выбираться первый из них. Следующие по списку синхронные серверы будут задействованы в случае сбоя первого.</P
><P
>Когда к ведущему серверу впервые присоединяется резервный, он ещё не будет полностью синхронизированным. Это называется состоянием <TT
CLASS="LITERAL"
>навёрстывания</TT
>. Как только отставание резервного от ведущего сервера сократится до нуля в первый раз, система перейдет в состояние <TT
CLASS="LITERAL"
>потоковой передачи</TT
> в реальном времени. Сразу после создания резервного сервера навёрстывание может быть длительным. В случае выключения резервного сервера длительность этого процесса увеличится соответственно продолжительности простоя. Резервный сервер может стать синхронным только по достижении состояния <TT
CLASS="LITERAL"
>потоковой передачи</TT
>.</P
><P
>Если ведущий сервер перезапускается при наличии зафиксированных транзакций, ожидающих подтверждения, эти транзакции будут помечены как полностью зафиксированные после восстановления ведущего. При этом нельзя гарантировать, что все резервные серверы успели получить все текущие данные WAL к моменту падения ведущего. Таким образом, некоторые транзакции могут считаться незафиксированными на резервном сервере, даже если они считаются зафиксированными на ведущем. Гарантия, которую мы можем дать, состоит в том, что приложение не получит явного подтверждения успешной фиксации, пока не будет уверенности, что резервный получил данные WAL.</P
><P
>Если действительно был утрачен последний резервный сервер, то следует отменить <TT
CLASS="VARNAME"
>synchronous_standby_names</TT
> и перезагрузить файл конфигурации на ведущем.</P
><P
>В случае если ведущий сервер стал недоступным для оставшихся резервных, следует переключиться на наиболее подходящий из имеющихся резервных серверов.</P
><P
>Если необходимо пересоздать резервный сервер при наличии ожидающей подтверждения транзакции необходимо убедиться, что команды pg_start_backup() и pg_stop_backup() запускаются в сессии с установленным <TT
CLASS="VARNAME"
>synchronous_commit</TT
> = <TT
CLASS="LITERAL"
>off</TT
>, в противном случае эти запросы на подтверждение будут бесконечными для вновь возникшего резервного сервера.</P
></DIV
></DIV
></DIV
><DIV
CLASS="NAVFOOTER"
><HR
ALIGN="LEFT"
WIDTH="100%"><TABLE
SUMMARY="Footer navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
><A
HREF="different-replication-solutions.html"
ACCESSKEY="P"
>Пред.</A
></TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="index.html"
ACCESSKEY="H"
>Начало</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
><A
HREF="warm-standby-failover.html"
ACCESSKEY="N"
>След.</A
></TD
></TR
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
>Сравнение различных решений</TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="high-availability.html"
ACCESSKEY="U"
>Уровень выше</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
>Отработка отказа</TD
></TR
></TABLE
></DIV
></BODY
></HTML
>